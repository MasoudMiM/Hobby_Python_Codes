{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait as Wait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the keywords for the search\n",
    "query = 'Mechanical Engineer'  # chang to the keyword you want\n",
    "location_org = 'United States' # change to the location you want\n",
    "\n",
    "# getting the patch of the chrome driver\n",
    "chromedriver_path = '' # path to your chrome driver exe file (example: C:\\Users\\username\\Downloads\\chromedriver_win32\\chromedriver.exe)\n",
    "# Linkedin username and password\n",
    "UserName = '' # use your linkedin username\n",
    "Password = '' # use your linkedin password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the driver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options, executable_path=chromedriver_path)\n",
    "driver.implicitly_wait(10)\n",
    "wait = Wait(driver, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging in to the linkedin account\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "\n",
    "email_input = driver.find_element(By.ID, 'username')\n",
    "password_input = driver.find_element(By.ID, 'password')\n",
    "email_input.send_keys(UserName)\n",
    "password_input.send_keys(Password)\n",
    "password_input.send_keys(Keys.ENTER)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going over a loop for searching the job postings\n",
    "total_pages = 40\n",
    "for page_num in range(1, total_pages):\n",
    "    print(f'page {page_num} ...')\n",
    "    url = f'https://www.linkedin.com/jobs/search/?keywords={query}&location={location_org}&start={25 * (page_num - 1)}'\n",
    "    driver.get(url)\n",
    "    \n",
    "    job_title_list = []\n",
    "    company_name_list = []\n",
    "    job_desc_list = []\n",
    "    job_id_list = []\n",
    "\n",
    "    # job id collection\n",
    "    job_ids = []\n",
    "\n",
    "\n",
    "    # scrolling down the page and collecting the data - these scrolling steps are based on the screen size and selected randomly\n",
    "    # they are not optimized and can be changed based on the screen size \n",
    "    steps_scrolling = [500, 1000, 1400, 1800, 2200, 2400, 2700, 3000, 3300, 3500]\n",
    "\n",
    "    for step in steps_scrolling:\n",
    "        driver.get(url)\n",
    "        time.sleep(10)  # Wait for 5 seconds\n",
    "        element = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main\"]/div/div[1]/div')))\n",
    "        driver.execute_script(f\"arguments[0].scroll(0, {step});\", element)\n",
    "\n",
    "        time.sleep(10)  # Wait for 5 seconds\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "\n",
    "        # let's extract the job title for the jobs in the page\n",
    "        job_title = soup.find_all('div', {'class': 'full-width artdeco-entity-lockup__title ember-view'})#.get_text().strip()\n",
    "        # let's extract the company name for the jobs in the page\n",
    "        company_name = soup.find_all('span', {'class': 'job-card-container__primary-description'})#.get_text().strip()\n",
    "        # let's extract the location for the jobs in the page\n",
    "        location_salary = soup.find_all('li', {'class': 'job-card-container__metadata-item'})#.get_text().strip()\n",
    "        # let's extract the job ids\n",
    "        ids_object = soup.find_all('li', {'class': 'ember-view jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item'})\n",
    "        job_ids = [ind['data-occludable-job-id'] for ind in ids_object]\n",
    "\n",
    "        for counter in range(len(job_ids)):\n",
    "            try:\n",
    "                job_title_list.append( job_title[counter].get_text().strip() )\n",
    "            except:\n",
    "                job_title_list.append(None)\n",
    "            \n",
    "            try:\n",
    "                company_name_list.append( company_name[counter].get_text().strip() )\n",
    "            except:\n",
    "                company_name_list.append(None)\n",
    "\n",
    "        # going over the job ids and collecting the job descriptions\n",
    "        for job_id in job_ids:\n",
    "            # checking if job id is already in the list and collect the job description if that is not the case\n",
    "            if job_id not in job_id_list:\n",
    "                job_id_list.append(job_id)\n",
    "                try:\n",
    "                    url_job = f'https://www.linkedin.com/jobs/search/?currentJobId={job_id}&keywords={query}&location={location_org}&start={25 * (page_num - 1)}'\n",
    "                    driver.get(url_job)\n",
    "                    time.sleep(5)  # Wait for 5 seconds\n",
    "                    soup_job = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    job_desc = soup_job.find_all('div', {'class': 'jobs-search__job-details--container'})\n",
    "                    job_desc = job_desc[0].find('article', class_='jobs-description__container m4').get_text().strip()\n",
    "                    job_desc_list.append(job_desc)\n",
    "                except:\n",
    "                    job_desc_list.append(None)\n",
    "                    continue\n",
    "            else:\n",
    "                job_id_list.append(None)\n",
    "                job_desc_list.append(None)\n",
    "                continue\n",
    "\n",
    "    # creating a dataframe from these lists\n",
    "    df_tmp = pd.DataFrame({'job_title': job_title_list,\n",
    "                        'company_name': company_name_list,\n",
    "                        'job_desc': job_desc_list,\n",
    "                        'job_id': job_id_list\n",
    "                        })\n",
    "\n",
    "    # dropping the duplicates based on the job title, company name, and location\n",
    "    df_tmp.drop_duplicates(subset=['job_title', 'company_name'], inplace=True)\n",
    "    df_tmp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # let's add the dataframe to the main dataframe\n",
    "    if page_num == 1:\n",
    "        df = df_tmp\n",
    "    else:\n",
    "        df = pd.concat([df, df_tmp], axis=0)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe as a scv file\n",
    "df.to_csv(f'{query}_{location_org}_jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(df_input, n_top = 10):\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    # plotting n_top number of jobs\n",
    "    plt.figure(figsize=(10,5))\n",
    "    x_plot = df_input['job_title'].value_counts().index\n",
    "    sns.barplot(x=x_plot[:n_top], y=df_input['job_title'].value_counts()[:n_top], color='b')\n",
    "    plt.xlabel('Job Title')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=75)\n",
    "\n",
    "    # plotting n_top number of companies\n",
    "    x_plot = df_input['company_name'].value_counts().index\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x=x_plot[:n_top], y=df_input['company_name'].value_counts()[:n_top], color='b')\n",
    "    plt.xlabel('Hiring Institution Name')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=75)\n",
    "\n",
    "    \n",
    "\n",
    "def text_analyzer(df_input, other_words_excluded, colum_name ,top_N=10): \n",
    "\n",
    "    txt = df_input[colum_name].str.replace(r'\\n', ' ', regex=True).str.replace(r'\\r', ' ', regex=True).str.replace(r'\\t', ' ', regex=True).str.lower().str.cat(sep=' ')\n",
    "    words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "    # excluding the stop words\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words_except_stop = [w for w in words if (w not in stopwords) and (w not in other_words_excluded)]\n",
    "\n",
    "    words_except_stop_dist = nltk.FreqDist(words_except_stop)\n",
    "\n",
    "    print('All frequencies, excluding Stopwords & Pubcuations:')\n",
    "    print('=' * 60)\n",
    "    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N), columns=['Word', 'Frequency'])\n",
    "    print(rslt)\n",
    "    print('=' * 60)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=rslt, x=\"Word\", y=\"Frequency\", color='b')\n",
    "    plt.xticks(rotation=80)\n",
    "    plt.title(f\"{top_N} most frequent words used\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots(df_input=df, n_top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding irrelevant words from the descriptions\n",
    "others = [\",\", \"mechanical\", \"engineering\", \"engineer\", \"'\", \"about\", \":\", \";\", \")\", \"(\",\"â€™\",\".\", \\\n",
    "          \"work\", \"job\", \"&\", \"including\", \"years\", \"skills\", \"team\", \"ability\", \\\n",
    "            \"development\", \"requirement\", \"requirements\", \"company\", \"required\", \"new\", \\\n",
    "                \"knowledge\", \"degree\", \"position\", \"related\", \"experience\", \"projects\", \"products\", \"system\",\\\n",
    "                    \"working\", \"status\", \"employment\", \"opportunity\", \"opportunities\", \"role\", \"benefits\",\\\n",
    "                      \"test\", \"$\", \"develop\", \"-\", \"'s\", \"designs\", \"engineers\",\"information\", \"and/or\", \"may\",\\\n",
    "                        \"provide\", \"must\", \"qualifications\", \"industry\", \"environment\",\n",
    "                        \"apply\", \"time\", \"processes\", \"disability\", \"!\"]\n",
    "text_analyzer(df_input=df, other_words_excluded=others, colum_name='job_desc' ,top_N=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
